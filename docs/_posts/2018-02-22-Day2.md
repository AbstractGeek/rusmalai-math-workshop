---
layout: post
title: Probability Distribution functions
date:   2018-02-15 14:00:00 +0530
categories: ["concepts"]
---


## Session 2: 22 February 2018 - Distribution functions and how they come about
	- Examples of discrete variables (probability mass function)
	- Examples of continuous variables (probability distribution functions)
	- Benefits and caveats of using Cumulative Distribution Functions (CDFs)

### A quick recap:

 - Probability space $ (\Omega,p) $

 - Dependence and Independence

 - Continuous versus Discrete

## Probability Mass/Distribution Functions


### Random variable

- A variable $ X $ that can take the values/realizations $ x $ from a domain/set $ \Omega $ (which has elements $ \omega $). Previously we were calling everything an 'event' and assigning a probability to that 'event'.

- There is a probability attached to this random variable which is given by the probability distribution function

#### The point is 'Everything is a Random Variable'  (Well, kinda)

- probability distribution function is a function that takes this random variable as an input and ouputs a number such that when summed over the domain/set it all adds up to 1

- I know....that was unnecessarily complicated!


Let's do it separately for discrete and continuous random variables:

### PMF for a discrete random variable

PMF stands for Probablity Mass Function $ P(x) $ and this is what it is:
- it is exactly the probability of the random variable happening
For every $ x \in \Omega $, there is a $ P(x) \geq 0 $ such that
$$ \sum_{x\in\Omega} P(x) = 1 $$

### PDF for a continuous random variable

PDF stands for Probability Density Function $ p(x) $ and there is a good reason it is called a 'density function' and not a 'mass function'
- each kutti (little) element $ dx $ between $ x $ and $ x+dx $ has a probability of $ p(x)dx $ being sampled
- these probabilities added up (in this case integrated out) give 1
$$ \int_{x\in\Omega} p(x) dx = 1 $$

## Expectation

#### What is the expected outcome of an experiment ?



Since each experiment gives us different realizations of the random variable $ X $,
#### In other words, what is the expected  value of the random variable $X$ ?


for the discrete case
$$ E[x] = \sum_{x\in\Omega} x P(x) $$
or for the coninuous case
$$ E[x] = \int_{x\in\Omega} x p(x) dx $$


## Uniform distribution

Let's list some examples

### Discete

- The example of sampling one nucleotide from the total pool of nucleotides in all genetic sequences (both strands included)
- ?

### Continuous

- Dart board (sampling matters)
- ?

### Discrete
$$ \Omega = \{a_1,a_2,a_3,....,a_n\} $$
$$ P(x) = \frac{1}{n} \qquad \text{where} \quad x\in\Omega$$

### Continuous
$$ \Omega = (a,b) $$
$$ p(x) = \frac{1}{b-a} \qquad \text{where} \quad x\in\Omega$$

Do these follow all the rules of Probability?

### Example: matching 2 sequences at random

Given any two sequences of equal length what is expected number of positions that match?


```python
import numpy as np
nucleotides = ['A','T','G','C']
neq=0
for k in range(0,1000):
    n = 100 #length of the sequences
    array1 = np.random.randint(4,size=n)
    array2 = np.random.randint(4,size=n)
    seq1 = ''
    seq2 = ''
#     print(array1)
#     print(array2)
    for i in array1:
        seq1 = seq1 + nucleotides[i]

    for j in array2:
        seq2 = seq2 + nucleotides[j]

#     print(seq1)
#     print(seq2)

    neq = neq + n - np.count_nonzero(array1-array2)

neq = neq/1000
print(neq)
fraction = neq/n
print(fraction)

```

    24.699
    0.24699000000000002


## Binomial Distribution

The above example was actually of a binomial distribution.

Whenever we sample something multiple times (with replacement) and ask for the chance of seeing **$m$** instances of a particular event **$E$**



For N independent and identically distributed (iid) random variable $ X \sim Ber(p) $, the probability that '0' comes up k times is


$$ P(k) = \binom{N}{k} p^k(1-p)^{N-k} $$

## Poisson Distribution

### Counting the stars in the sky

Let's choose a patch of sky and count the number of stars in it. What is the frequency/chance of seeing no stars? one star?...  

Let's work it out! Intuitively!

Let's not touch math for a while



## Gaussian Distribution

Where do we see this?



- A lot of independent factors shaping things up!

- Computing averages



```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

avg = np.zeros(1000)
for i in range(0,1000):
    x = np.random.normal(0,1,size=50)
    # print(x)
    avg[i] = np.mean(x)

plt.hist(avg,normed=True,bins=50)

```




    (array([ 0.05228988,  0.        ,  0.        ,  0.        ,  0.05228988,
             0.10457976,  0.        ,  0.05228988,  0.2614494 ,  0.47060892,
             0.41831904,  0.2614494 ,  0.47060892,  0.47060892,  0.88892795,
             0.83663807,  1.30724699,  1.62098627,  1.62098627,  1.77785591,
             2.14388506,  2.77136362,  2.5622041 ,  2.40533446,  3.86945109,
             2.98052314,  2.92823326,  3.50342193,  2.92823326,  1.72556603,
             1.93472555,  2.35304458,  1.72556603,  1.46411663,  1.15037735,
             0.83663807,  1.04579759,  0.99350771,  0.88892795,  0.15686964,
             0.31373928,  0.20915952,  0.36602916,  0.05228988,  0.10457976,
             0.10457976,  0.        ,  0.        ,  0.05228988,  0.05228988]),
     array([-0.48492563, -0.46580147, -0.44667731, -0.42755315, -0.40842899,
            -0.38930483, -0.37018067, -0.35105651, -0.33193235, -0.31280819,
            -0.29368403, -0.27455987, -0.25543571, -0.23631155, -0.21718739,
            -0.19806323, -0.17893907, -0.15981491, -0.14069075, -0.1215666 ,
            -0.10244244, -0.08331828, -0.06419412, -0.04506996, -0.0259458 ,
            -0.00682164,  0.01230252,  0.03142668,  0.05055084,  0.069675  ,
             0.08879916,  0.10792332,  0.12704748,  0.14617164,  0.1652958 ,
             0.18441996,  0.20354412,  0.22266828,  0.24179244,  0.2609166 ,
             0.28004076,  0.29916491,  0.31828907,  0.33741323,  0.35653739,
             0.37566155,  0.39478571,  0.41390987,  0.43303403,  0.45215819,
             0.47128235]),
     <a list of 50 Patch objects>)




![png]({{ site.baseurl }}/assets/2018-02-22-output_18_1.png)


## The road ahead

 -   Statistics
 -   Estimation and Inference
 -   Another approach to Probability: Bayesian approach
